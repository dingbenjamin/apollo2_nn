{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import sklearn\n",
    "import keras.utils\n",
    "import sys\n",
    "from keras import Sequential, regularizers\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_categorical(one_hot):\n",
    "    return [argmax(x) for x in one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(labels):\n",
    "    return [1 if x == 1 else 0 for x in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Seizure.csv').drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = df[\"y\"]\n",
    "x_all = df.drop(columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = sklearn.preprocessing.scale(x_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train/test setes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_array, y_all, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index class labels from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_split -= 1\n",
    "# y_test_split -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(88, \n",
    "#                 activation='sigmoid', \n",
    "#                 input_dim=178))\n",
    "# model.add(Dense(88, activation='sigmoid'))\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#                 optimizer=keras.optimizers.Adadelta(),\n",
    "#                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_split_one_hot = keras.utils.to_categorical(y_train_split, num_classes=5)\n",
    "# y_test_split_one_hot = keras.utils.to_categorical(y_test_split, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_split, y_train_split_one_hot, epochs=32, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_split_predict = from_categorical(model.predict(x_test_split))\n",
    "# acc = accuracy_score(y_test_split_predict, y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a good random start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9781291172595521\n"
     ]
    }
   ],
   "source": [
    "while acc < 0.971:\n",
    "    model_binary = Sequential()\n",
    "    model_binary.add(Dense(88, \n",
    "                    activation='sigmoid', \n",
    "                    input_dim=178,\n",
    "                    use_bias=True))\n",
    "    model_binary.add(Dense(2, activation='softmax', use_bias=True))\n",
    "    model_binary.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=keras.optimizers.Adadelta(),\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    y_train_split_binary = keras.utils.to_categorical(binarize(y_train_split), num_classes=2)\n",
    "    y_test_split_binary = binarize(y_test_split)\n",
    "    \n",
    "    model_binary.fit(x_train_split, y_train_split_binary, epochs=31, verbose=0)\n",
    "    \n",
    "    y_test_split_predict_binary = from_categorical(model_binary.predict(x_test_split))\n",
    "    acc = accuracy_score(y_test_split_predict_binary, y_test_split_binary)\n",
    "    print(acc)\n",
    "    cm = confusion_matrix(y_test_split_predict_binary, y_test_split_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary.save_weights('model_binary.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate C code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import h5py\n",
    "\n",
    "def print_structure(weight_file_path):\n",
    "    \"\"\"\n",
    "    Prints out the structure of HDF5 file.\n",
    "\n",
    "    Args:\n",
    "      weight_file_path (str) : Path to the file to analyze\n",
    "    \"\"\"\n",
    "    f = h5py.File(weight_file_path)\n",
    "    try:\n",
    "        if len(f.attrs.items()):\n",
    "            print(\"{} contains: \".format(weight_file_path))\n",
    "            print(\"Root attributes:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(\"  {}: {}\".format(key, value))\n",
    "\n",
    "        if len(f.items())==0:\n",
    "            return \n",
    "\n",
    "        for layer, g in f.items():\n",
    "            print(\"  {}\".format(layer))\n",
    "            print(\"    Attributes:\")\n",
    "            for key, value in g.attrs.items():\n",
    "                print(\"      {}: {}\".format(key, value))\n",
    "\n",
    "            print(\"    Dataset:\")\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                subkeys = param.keys()\n",
    "                for k_name in param.keys():\n",
    "                    print(\"      {}/{}: {}\".format(p_name, k_name, param.get(k_name)[:]))\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_structure(weight_file_path):\n",
    "    f = h5py.File(weight_file_path)\n",
    "    weights = []\n",
    "    try:\n",
    "        if len(f.items())==0:\n",
    "            return \n",
    "\n",
    "        for layer, g in f.items():\n",
    "            for p_name in g.keys():\n",
    "                param = g[p_name]\n",
    "                subkeys = param.keys()\n",
    "                for k_name in param.keys():\n",
    "                    weights.append(param.get(k_name)[:])\n",
    "    finally:\n",
    "        f.close()\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neurons(model):\n",
    "    weights = model.get_weights()\n",
    "    neurons = []\n",
    "    for weight_matrix in weights:\n",
    "        neurons.append(weight_matrix.shape[0])\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(model):\n",
    "    return len(neurons(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(model):\n",
    "    weights = model.get_weights()\n",
    "    return sum([np.prod(layer.shape) for layer in weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_string(first_connection, last_connection, \n",
    "                 activation_steepness=0, activation_function=6):\n",
    "    string = \"{\" + str(first_connection) + \\\n",
    "    \", \" + str(last_connection) + \\\n",
    "    \", \" + str(activation_steepness) + \\\n",
    "    \", \" + str(activation_function) + \"}\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neuron_array_string(model):\n",
    "    num_neurons = neurons(model)\n",
    "    neuron_array_string = 'fann_neuron fann_neurons[{}] = '.format(sum(num_neurons)) + '{'\n",
    "    for layer_index in range(len(num_neurons)):\n",
    "        # Input layer\n",
    "        if layer_index == 0:\n",
    "            for neuron in range(num_neurons[layer_index]):\n",
    "                neuron_array_string += neuron_string(0, 0, 0, 0) + \", \"\n",
    "            total_neurons = num_neurons[0]\n",
    "        else:\n",
    "            for neuron in range(num_neurons[layer_index]):\n",
    "                start_neuron = total_neurons\n",
    "                end_neuron = total_neurons + num_neurons[layer_index-1]\n",
    "                neuron_array_string += neuron_string(start_neuron, end_neuron, 1.00, 6) + \", \"\n",
    "                total_neurons = end_neuron\n",
    "    neuron_array_string = neuron_array_string[:-2] + '};'\n",
    "    return neuron_array_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_layer_array_string(model):\n",
    "    num_neurons = neurons(model)\n",
    "    layer_array_string = 'fann_layer fann_layers[{}] = '.format(len(num_neurons)) + '{'\n",
    "    total_neurons = 0\n",
    "    for i in range(len(num_neurons)):\n",
    "        start, end = total_neurons, total_neurons + num_neurons[i]\n",
    "        layer_array_string += '{' + '{}, {}'.format(start, end) + '}, '\n",
    "        total_neurons = end\n",
    "    layer_array_string = layer_array_string[:-2] + '};'\n",
    "    return layer_array_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_weight_array_string(model):\n",
    "    weights = model.get_weights()\n",
    "    total_weights = sum([np.prod(layer.shape) for layer in weights])\n",
    "    weight_array_string = 'fann_type fann_weights[{}] = '.format(total_weights) + '{'\n",
    "    for weight_matrix in weights:\n",
    "        if len(weight_matrix.shape) == 1:\n",
    "            for i in range(len(weight_matrix)):\n",
    "                weight_array_string += str(weight_matrix[i]) + ', '\n",
    "        elif len(weight_matrix.shape) == 2:\n",
    "            for i in range(weight_matrix.shape[0]):\n",
    "                for j in range(weight_matrix.shape[1]):\n",
    "                    weight_array_string += str(weight_matrix[i][j]) + ', '\n",
    "        else:\n",
    "            raise Exception('Weight matrix shape is incorrect')\n",
    "    weight_array_string = weight_array_string[:-2] + '};'\n",
    "    return weight_array_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fann_net.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fann_net_h(model):\n",
    "    try:\n",
    "        num_neurons = sum(neurons(model))\n",
    "        num_weights = weights(model)\n",
    "        num_layers = layers(model)        \n",
    "        f = open('fann_net.h', 'w')\n",
    "        f.write('#ifndef FANN_FANN_NET_H_\\n')\n",
    "        f.write('#define FANN_FANN_NET_H_\\n\\n')\n",
    "        f.write('#include \"fann.h\"\\n')\n",
    "        f.write('#include \"fann_structs.h\"\\n\\n')\n",
    "        f.write('extern const enum fann_nettype_enum network_type;\\n\\n')\n",
    "        f.write('extern fann_neuron fann_neurons[' + str(num_neurons) + '];\\n\\n')\n",
    "        f.write('extern fann_type fann_weights[' + str(num_weights) + '];\\n\\n')\n",
    "        f.write('extern fann_layer fann_layers[' + str(num_layers) + '];\\n\\n')\n",
    "        f.write('#endif // FANN_FANN_NET_H')\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fann_net.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fann_net_c(model):\n",
    "    try:\n",
    "        f = open('fann_net.c', 'w')\n",
    "        f.write('#include \"fann_net.h\"\\n\\n')\n",
    "        f.write('const enum fann_nettype_enum network_type = 0;\\n\\n')\n",
    "        neuron_array_string = gen_neuron_array_string(model)\n",
    "        weight_array_string = gen_weight_array_string(model) \n",
    "        layer_array_string = gen_layer_array_string(model)\n",
    "        f.write(neuron_array_string + \"\\n\\n\")\n",
    "        f.write(weight_array_string + \"\\n\\n\")\n",
    "        f.write(layer_array_string + \"\\n\\n\")\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Generate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_fann_net_h(model_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_fann_net_c(model_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
